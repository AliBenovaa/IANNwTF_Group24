{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpjKK4Tn7s+tzywx6gbtbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliBenovaa/IANNwTF_Group24/blob/main/Homework02Group24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8hV0GT9PQFNE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu_function(x):\n",
        "    return (np.maximum(0,x))\n",
        "\n",
        "def relu_derivation(x):\n",
        "    return 0 if x < 0 else 1\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Implements a perceptron network\"\"\"\n",
        "    def __init__(self,input_units):\n",
        "        self.W = np.array(np.random.randn(input_units)) #constructs a weight matrix with random values\n",
        "        self.b = np.zeros((1)) #constructs a bias with value zero\n",
        "        self.input_units=input_units\n",
        "        self.layer_input = None\n",
        "        self.layer_preact = None\n",
        "        self.layer_act = None\n",
        "        self.learning_rate = 0.001 #sets the learning rate \n",
        "\n",
        "\n",
        "\n",
        "    def forward_step(self, inputs):\n",
        "        \"\"\"\n",
        "        Single Layer Forward Propagation\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        self.output = relu_function(np.sum(np.dot(self.W, inputs))+self.b)  #apply the relu_function to each of the unit's parameters\n",
        "        return self.output\n",
        "\n",
        "\n",
        "\n",
        "    def backward_step(self, delta):\n",
        "        \"\"\"\n",
        "        Update all weights and bias with the one delta multiplied by the matching input/activation multiplied with the learning rate (alpha).\n",
        "        :parameter delta is the value of the delta-rule\n",
        "        \"\"\"\n",
        "\n",
        "        #Calculating the gradient and then updating the parameters\n",
        "        self.W -= self.learning_rate * delta * self.inputs \n",
        "        self.b -= self.learning_rate * delta \n",
        "        \n",
        "\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_mean_accuracy(outputs, targets):\n",
        "    if len(outputs) != len(targets):\n",
        "        raise IndexError(\"Length of outputs does not match target length!\")\n",
        "    correct = 0\n",
        "    for i, output in enumerate(outputs):\n",
        "        if abs(output-targets[i]) < 0.5:\n",
        "            correct += 1\n",
        "    return float(correct)/float(len(outputs))\n",
        "\n",
        "def calc_mean_loss(outputs, targets):\n",
        "    if len(outputs) != len(targets):\n",
        "        raise IndexError(\"Length of outputs does not match target length!\")\n",
        "    loss_sum = 0.0\n",
        "    for i, output in enumerate(outputs):\n",
        "        loss_sum += abs(output-targets[i])\n",
        "    return float(loss_sum)/float(len(outputs))"
      ],
      "metadata": {
        "id": "5LxE9rfXQPPG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP():\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        :param layers is an array, containing input, hidden and output layers. ([input, h1, ..., hn, output])\n",
        "        It's length-2 is the number of hidden layers.\n",
        "        The elements are the number of neurons on the respective layer.\n",
        "        \"\"\"\n",
        "        # input layer is not really made up of perceptrons but instead only describes the input vector\n",
        "        # still remember how many inputs there are\n",
        "        self.input_number = layers[0]\n",
        "\n",
        "        # initialize hidden layers\n",
        "        hlayers = []\n",
        "        for i in range(1, len(layers)-1):\n",
        "            hlayers.append([])\n",
        "            for j in range(layers[i]):\n",
        "                hlayers[-1].append(Layer(layers[i-1]))\n",
        "        self.hidden_layers = np.array(hlayers)\n",
        "\n",
        "        # initialize output layer\n",
        "        self.output_neurons = np.array([Layer(layers[-2]) for _ in range(layers[-1])])\n",
        "\n",
        "        self.output = np.empty(layers[-1])\n",
        "\n",
        "\n",
        "    def forward_step(self, input):\n",
        "        \"\"\"\n",
        "        Calculate output of the MLP.\n",
        "        :param input to the MLP\n",
        "        \"\"\"\n",
        "        next_vals = input\n",
        "        \n",
        "        # loop through hidden layers to do the forward_step for each layer hl\n",
        "        for hl in self.hidden_layers:\n",
        "            vals = np.copy(next_vals)\n",
        "            next_vals = np.array([p.forward_step(vals) for p in hl])\n",
        "            \n",
        "        next_vals=next_vals.flatten()\n",
        "        # return output of output layer\n",
        "        self.output = np.array([p.forward_step(next_vals) for p in self.output_neurons])\n",
        "        return self.output\n",
        "\n",
        "    def backprop_step(self, target):\n",
        "        \"\"\"\n",
        "        Use loss function and backpropagation to change Perceptron weights.\n",
        "        :param target is the expected output.\n",
        "        \"\"\"\n",
        "        # calculate delta for every layer and call .update(delta)\n",
        "        deltas = np.empty(len(self.output_neurons))\n",
        "        \n",
        "        # for output layer: delta = -(ti-yi)*relu'(diN)\n",
        "        efunc = -1.0 * (target-self.output)\n",
        "        for i, n_out in enumerate(self.output_neurons):\n",
        "            deltas[i] = efunc[i] * relu_derivation(n_out.output)\n",
        "            n_out.backward_step(deltas[i])\n",
        "\n",
        "        # for other layers\n",
        "        nextlayer = self.output_neurons\n",
        "        nextsize = len(self.output_neurons)\n",
        "        for l in range(len(self.hidden_layers)-1, -1, -1):\n",
        "            nextdeltas = np.empty(len(self.hidden_layers[l]))\n",
        "            # for each perceptron in this hidden layer\n",
        "            for i in range(len(self.hidden_layers[l])):\n",
        "                esum = 0\n",
        "                for k in range(nextsize):\n",
        "                    esum += deltas[k] * nextlayer[k].W[i]\n",
        "\n",
        "                nextdeltas[i] = esum * relu_derivation(self.hidden_layers[l][i].output)\n",
        "                self.hidden_layers[l][i].backward_step(nextdeltas[i])\n",
        "\n",
        "            nextsize = len(self.hidden_layers[l])\n",
        "            nextlayer = self.hidden_layers[l]\n",
        "\n",
        "            deltas = nextdeltas.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0VpHwrc1QSKX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is the main script for executing the self-implemented MLP on learning logical operations.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This is the main script for executing the self-implemented MLP on learning logical operations.\n",
        "\"\"\"\n",
        "#   Taks 1 Our data set\n",
        "x = np.random.uniform(0,1,100)\n",
        "t = np.empty(100)\n",
        "\n",
        "for i in range(len(x)):\n",
        "    targets = ((x[i] ** 3) - (x[i] ** 2 ))+1\n",
        "    t[i] = targets\n",
        "\n",
        "\n",
        "\n",
        "# create a MLP with 100 inputs, one hidden layers with 10 perceptrons each and one output neuron\n",
        "mlp = MLP([1, 10, 1])\n",
        "\n",
        "analysis = []\n",
        "\n",
        "\n",
        "input = x\n",
        "target = t\n",
        "\n",
        "for i in range(1000): #for 1000 epochs\n",
        "    outputs = []\n",
        "\n",
        "    for j in range(100):\n",
        "        outputs.append(mlp.forward_step(input[j]))\n",
        "        mlp.backprop_step(target[j])\n",
        "    \n",
        "    # keep track of accuracy and loss for later visualization\n",
        "    mean_accuracy = calc_mean_accuracy(outputs, target)\n",
        "    mean_loss = calc_mean_loss(outputs, target)\n",
        "    analysis.append([i, mean_accuracy, mean_loss])\n",
        "\n",
        "analysis = np.array(analysis)\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "\n",
        "ax1.plot(analysis[:,0], analysis[:,1])\n",
        "ax1.set_ylabel(\"Average Accuracy\")\n",
        "\n",
        "ax2.plot(analysis[:,0], analysis[:,2])\n",
        "ax2.set_ylabel(\"Average Loss\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Doing one test:\")\n",
        "output = mlp.forward_step(input[0])\n",
        "print(f\"Output for input {input[0]}: {output}\")\n",
        "output = mlp.forward_step(input[1])\n",
        "print(f\"Output for input {input[1]}: {output}\")\n",
        "output = mlp.forward_step(input[2])\n",
        "print(f\"Output for input {input[2]}: {output}\")\n",
        "output = mlp.forward_step(input[3])\n",
        "print(f\"Output for input {input[3]}: {output}\")\n",
        "output = mlp.forward_step(input[4])\n",
        "print(f\"Output for input {input[4]}: {output}\")\n",
        "output = mlp.forward_step(input[5])\n",
        "print(f\"Output for input {input[5]}: {output}\")\n",
        "output = mlp.forward_step(input[6])\n",
        "print(f\"Output for input {input[6]}: {output}\")\n",
        "output = mlp.forward_step(input[7])\n",
        "print(f\"Output for input {input[7]}: {output}\")\n",
        "output = mlp.forward_step(input[8])\n",
        "print(f\"Output for input {input[8]}: {output}\")\n",
        "output = mlp.forward_step(input[9])\n",
        "print(f\"Output for input {input[9]}: {output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "rj64KYf_QbzU",
        "outputId": "25fe7433-52e8-4fb0-a766-f34a30b977b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZRdZZnv8e+vTmWABCEhAdMkkBCiNtI2YGRQ+i4bZBAR2qEhaWmZunO1QdCrrXBVsGn1tjZLaZVFExRERKZoa+DSRCax+6pIRRGZIiEIJIIppgABklTVc//Y76k6VedU6lTtM+Sc+n3WOqv2fvdwnl07q568w363IgIzM7OhOpodgJmZbZucIMzMrCInCDMzq8gJwszMKnKCMDOzijqbHUCtzJgxI+bOndvsMMzMWsrKlSufjoiZlba1TYKYO3cuXV1dzQ7DzKylSHpsuG1NaWKSdJmk9ZLuG2a7JH1N0mpJ90rav9ExmpmNd83qg/g2cNRWtr8TWJA+S4CLGxCTmZmVaEoTU0T8VNLcrexyHPCdyB7z/oWknSTNiogn6xXTJ5f9hp+s6q7X6c3M6maf3XbkspPfUvPzbqt9ELsBT5Ssr01lgxKEpCVkNQx23333XF/4/1Y/w5RJnRy05/Rc5zEza7TZ07avy3m31QRRlYhYCiwFWLhwYa5JpXr7ggPmTuf/vPdNNYnNzKzV5eqDkPQRSdNqFUyJdcCckvXZqaxueiPo6FA9v8LMrKXk7aTeFbhb0nWSjpJUq7+wy4EPptFMBwEb6tn/AFkNotMJwsysX64EERGfIRtp9C3gZOBhSV+UNH9rx0m6Gvg58HpJayWdJulDkj6UdrkJWAOsBi4F/iFPnNXo6e2j4ARhZtYvdx9ERISkp4CngB5gGrBM0i0R8clhjlk80jmB0/PGNhp9gROEmVmJXAlC0lnAB4GngW8C/xgRWyR1AA8DFRPEtqinr89NTGZmJfLWIKYD742IQY9qR0SfpGNynruh+vpwJ7WZWYm8ndT/CTxbXJH0GkkHAkTEgznP3VCuQZiZDZY3QVwMvFSy/hItOC1GRNAX0FGzQVhmZq0vb4JQ6lAGsqYlWvDhu96+7BJcgzAzG5A3QayRdKakCelzFtnw1JbSkxJEoeAEYWZWlDdBfAh4K9lTzmuBA0lzI7WSvlQJKriJycysX67moIhYDyyqUSxN01+DcBOTmVm/vM9BTAZOA94ITC6WR8SpOeNqqN5eJwgzs6HyNjFdCbwWOBK4k2xSvRfzBtVoveFOajOzofImiL0i4rPAxoi4AngXWT9ES+ntb2Jq1gv2zMy2PXn/Im5JP5+XtA+wI7BLznM23EAfRJMDMTPbhuR9ZmFpeh/EZ8im6J4KfDZ3VA3W5xqEmVmZMSeINCHfCxHxHPBTYM+aRdVgrkGYmZUb85/E9NR0y8zWujXugzAzK5f3L+Ktkj4haY6k6cVPTSJroP4E4QflzMz65e2DOCH9LH25T9BizU1BliCcH8zMBuR9knperQJppuJ0g84PZmYD8j5J/cFK5RHxnTznbbTiXExyFcLMrF/eJqa3lCxPBg4DfgW0VILor0E4P5iZ9cvbxPSR0nVJOwHX5IqoiZwfzMwG1Hpc50ag5folBmoQThFmZkV5+yBuAIpvlOsA9gauyxtUo/WPYmpyHGZm25K8fRAXlCz3AI9FxNqc52w490GYmZXLmyAeB56MiFcBJG0naW5E/D53ZA3UXwVyhjAz65e3D+J6oK9kvTeVtZQ+PwhhZlYmb4LojIjNxZW0PDHnORvO+cHMrFzeBNEt6djiiqTjgKdznrMJ/KCcmdlQefsgPgRcJekbaX0tUPHp6m2ZaxBmZuXyPij3CHCQpKlp/aWaRNVgxU5qVyDMzAbkamKS9EVJO0XESxHxkqRpkj5fq+AapViD8CgmM7MBefsg3hkRzxdX0tvljs55zobrn6yvyXGYmW1L8iaIgqRJxRVJ2wGTtrL/Nin625iaGoaZ2TYlbyf1VcBtki5P66fQYjO5QulUG84QZmZFeTupvyTpN8A7UtE/R8SK/GE1mKfaMDMrk7cGQUTcDNwsaQrwXkn/NyLelT+0xnELk5lZubyjmCZKeo+k64EngUOBf6/iuKMkrZK0WtLZFbafLKlb0j3p83d54hyJp/s2Mys3phqEpCOAxcARwB1k/Q5viYhTqji2AFwEHE72YN3dkpZHxANDdr02Is4YS3yjVRzF1OH8YGbWb6w1iJuBPYFDIuLEiLiBwZP2bc0BwOqIWJPmbroGOG6McdSEH5QzMys31gSxP/Bz4FZJt0g6DShUeexuwBMl62tT2VDvk3SvpGWS5lQ6kaQlkrokdXV3d48m/kHC41zNzMqMKUFExD0RcXZEzAfOA/YFJkj6T0lLahDXDcDciHgTcAtwxTBxLI2IhRGxcObMmWP+MtcgzMzK5X4ndUT8LCI+AswGvgocNMIh64DSGsHsVFZ6zmciYlNa/Sbw5rxxbpUn6zMzK5M7QRRFRF9E/DgiTh1h17uBBZLmSZoILAKWl+4gaVbJ6rHAg7WKs5LwdN9mZmVyPwcxWhHRI+kMYAVZv8VlEXG/pPOBrohYDpyZ3jPRAzwLnFzfmLKfHsVkZjag4QkCICJuAm4aUnZuyfI5wDmNiqevv4nJGcLMrCh3E5OkQySdkpZnSpqXP6zGKo5icguTmdmAvE9Snwd8ioH/7U8Avps3qEaLkXcxMxt38tYg3kPWibwRICL+AOyQN6hGC0/WZ2ZWJm+C2BxZ+0wApAn7WpCn+zYzGypvgrhO0iXATpL+HrgVuDR/WI3VP4qpZoN+zcxaX973QVwg6XDgBeD1wLkRcUtNImsgj2IyMytXi/dB3EI2HUbLGnhQrsmBmJltQ3IlCEkvUj4IaAPQBXw8ItbkOX+jhKfaMDMrk7cGcSHZbKzfI/v7ugiYD/wKuAx4e87zN4Qn6zMzK5e3W/bYiLgkIl6MiBciYilwZERcC0yrQXwN4em+zczK5U0QL0s6XlJH+hwPvJq2tdzzZ65BmJkNyJsgPgD8LbAe+GNaPlHSdkBDXhdaCwOvHHWGMDMryjvMdQ3w7mE2/3eeczeSO6nNzMrlHcU0GTgNeCMwuVhexTshtimeasPMrFzeJqYrgdcCRwJ3kr0d7sW8QTXaQBe1M4SZWVHeBLFXRHwW2BgRVwDvAg7MH1ZjebpvM7NyeRPElvTzeUn7ADsCu+Q8Z8O13HArM7MGyPug3FJJ04DPkL1Xeirw2dxRNVixBtHhd46amfUbc4KQ1AG8EBHPAT8F9qxZVA3mUUxmZuXG3MQUEX3AJ2sYS9N4qg0zs3J5+yBulfQJSXMkTS9+ahJZA4Wn+zYzK5O3D+KE9PP0krKgxZqbPN23mVm5vE9Sz6tVIM3kPggzs3K5mpgkbS/pM5KWpvUFko6pTWiNM9AH4RRhZlaUtw/icmAz8Na0vg74fM5zNpwflDMzK5c3QcyPiC+THpiLiJdpwZYaNzGZmZXLmyA2p6m9A0DSfGBT7qgabKAG4RRhZlaUdxTT54CbgTmSrgLeBpyc85wN5/fJmZmVyzuK6ceSVgIHkf19PSsinq5JZA3k6b7NzMrlfR/EDcD3gOURsbE2ITWep/s2MyuXtw/iAuAvgAckLZP0/vQSoZbS3weR97dhZtZG8jYx3QncKakAHAr8PXAZ8JoaxNYwHsVkZlYubyc1aRTTu8mm3dgfuCLvORttYKoNpwgzs6K8fRDXAQeQjWT6BnBnmuW1pbgGYWZWLm8N4lvA4ojoBZB0iKTFEXH6CMdtUzzdt5lZubx9ECsk7SdpMXA88Cjwg5pE1kCe7tvMrNyYxu1Iep2k8yQ9BHwdeAJQRPxlRHy9iuOPkrRK0mpJZ1fYPknStWn7XZLmjiXOavV5LiYzszJjHdj5ENmopWMi4pCUFHqrOTCNeLoIeCewN7BY0t5DdjsNeC4i9gK+CnxpjHGOKCJ44dUtKbZ6fYuZWesZa4J4L/AkcIekSyUdRvV9vAcAqyNiTURsBq4Bjhuyz3EMjIZaBhymOg0xenbjZi65cw177TKVSZ2FenyFmVlLGlMfRET8EPihpClkf8w/Cuwi6WLgPyLix1s5fDeyJqmitcCBw+0TET2SNgA7A4Om8ZC0BFgCsPvuu4/lUth+Yif/++g38Nb5M8Z0vJlZu8r17HBEbIyI70XEu4HZwK+BT9Uksuq+f2lELIyIhTNnzhzTObabWGDJ/5jPPrvtWOPozMxaW80ml4iI59If7MNG2HUdMKdkfXYqq7iPpE5gR+CZWsVqZmYja8bsQ3cDCyTNkzQRWAQsH7LPcuCktPx+4PYoTphkZmYNoWb83ZV0NHAhUAAui4gvSDof6IqI5WnCvyuB/YBngUURsWaEc3YDj+UIawZD+jjGAV9z+xtv1wu+5tHaIyIqttE3JUFsiyR1RcTCZsfRSL7m9jferhd8zbXkCa7NzKwiJwgzM6vICWLA0mYH0AS+5vY33q4XfM014z4IMzOryDUIMzOryAnCzMwqGvcJYqSpx1uVpDmS7pD0gKT7JZ2VyqdLukXSw+nntFQuSV9Lv4d7Je3f3CsYO0kFSb+WdGNan5emjV+dppGfmMobOq18vUjaSdIySQ9JelDSwe1+nyV9LP27vk/S1ZImt9t9lnSZpPWS7ispG/V9lXRS2v9hSSdV+q7hjOsEUeXU462qB/h4ROwNHAScnq7tbOC2iFgA3JbWIfsdLEifJcDFjQ+5Zs4CHixZ/xLw1TR9/HNk08lDA6eVr7N/A26OiDcAf0527W17nyXtBpwJLIyIfcgeuF1E+93nbwNHDSkb1X2VNB04j2xC1AOA84pJpSoRMW4/wMHAipL1c4Bzmh1Xna71R8DhwCpgViqbBaxKy5eQvT62uH//fq30IZvb6zay95XcSDYN/dNA59B7DqwADk7LnWk/NfsaRnm9O5K9yVFDytv2PjMw2/P0dN9uBI5sx/sMzAXuG+t9BRYDl5SUD9pvpM+4rkFQeerx3ZoUS92kKvV+wF3ArhHxZNr0FLBrWm6X38WFwCeBvrS+M/B8RPSk9dLrGjStPFCcVr6VzAO6gctTs9o30zT8bXufI2IdcAHwONl7aTYAK2nv+1w02vua636P9wTR9iRNBb4PfDQiXijdFtl/KdpmnLOkY4D1EbGy2bE0UCewP3BxROwHbGSg2QFoy/s8jew9NPOAPwGmUN4U0/YacV/He4KoZurxliVpAllyuCoifpCK/yhpVto+C1ifytvhd/E24FhJvyd7U+GhZO3zO6Vp42HwdbXDtPJrgbURcVdaX0aWMNr5Pr8DeDQiuiNiC/ADsnvfzve5aLT3Ndf9Hu8Jopqpx1uSJAHfAh6MiK+UbCqdSv0ksr6JYvkH02iIg4ANJVXZlhAR50TE7IiYS3Yvb4+IDwB3kE0bD+XX3NLTykfEU8ATkl6fig4DHqCN7zNZ09JBkrZP/86L19y297nEaO/rCuAISdNSzeuIVFadZnfCNPsDHA38DngE+HSz46nhdR1CVv28F7gnfY4ma3u9DXgYuBWYnvYX2YiuR4Dfko0Qafp15Lj+twM3puU9gV8Cq4HrgUmpfHJaX52279nsuMd4rfsCXele/xCY1u73Gfgn4CHgPrJXA0xqt/sMXE3Wx7KFrKZ42ljuK3BquvbVwCmjicFTbZiZWUXjvYnJzMyG4QRhZmYVOUGYmVlFnSPv0hpmzJgRc+fObXYYZmYtZeXKlU/HMO+kbpsEMXfuXLq6upodhplZS5H02HDb3MRkZmYVjfsE8eqWXm5/6I888ezLzQ7FzGybMu4TxEubejj1213csWr9yDubmY0j4z5BTJ5QAOCVzb1NjsTMbNviBNGZ/Qpe3dI3wp5mZuPLuE8QnYUOJhTEqz2uQZiZlRr3CQJgcmeBV7c4QZiZlXKCACZNKLiJycxsCCcIYPKEDja5BmFmNogTBNlIplecIMzMBnGCALab4D4IM7OhnCCAiZ0dbOn1i5PMzEo5QQCFDtHT505qM7NSThBAZ4fo7XMNwsyslBMExRqEE4SZWSknCFyDMDOrZMQEIWm+pElp+e2SzpS0UzUnl3SUpFWSVks6u8L2r0q6J31+J+n5km29JduWj+aiRqvQ4U5qM7Ohqnmj3PeBhZL2ApYCPwK+Bxy9tYMkFYCLgMOBtcDdkpZHxAPFfSLiYyX7fwTYr+QUr0TEvtVeSB5ZDcKd1GZmpappYuqLiB7gPcDXI+IfgVlVHHcAsDoi1kTEZuAa4Lit7L8YuLqK89ZcZ8F9EGZmQ1WTILZIWgycBNyYyiZUcdxuwBMl62tTWRlJewDzgNtLiidL6pL0C0l/NcxxS9I+Xd3d3VWEVJn7IMzMylWTIE4BDga+EBGPSpoHXFnjOBYByyKi9HHmPSJiIfA3wIWS5g89KCKWRsTCiFg4c+bMMX95oaODHvdBmJkNMmIfROozOBNA0jRgh4j4UhXnXgfMKVmfncoqWQScPuR716WfayT9hKx/4pEqvnfUXIMwMytXzSimn0h6jaTpwK+ASyV9pYpz3w0skDRP0kSyJFA2GknSG4BpwM9LyqaVjJyaAbwNeGDosbVSKPhJajOzoappYtoxIl4A3gt8JyIOBN4x0kGpY/sMYAXwIHBdRNwv6XxJx5bsugi4JiJK/wv/p0CXpN8AdwD/Ujr6qdY6/aCcmVmZaoa5dkqaBRwPfHo0J4+Im4CbhpSdO2T9cxWO+xnwZ6P5rjw6OzrodR+Emdkg1dQgzierBTwSEXdL2hN4uL5hNZaHuZqZlaumk/p64PqS9TXA++oZVKMV3EltZlammk7q2ZL+Q9L69Pm+pNmNCK5ROj3dt5lZmWqamC4nG330J+lzQyprG4UO0RfQ51qEmVm/ahLEzIi4PCJ60ufbwNifStsGdXYIwP0QZmYlqkkQz0g6UVIhfU4Enql3YI1U6Mh+De6HMDMbUE2COJVsiOtTwJPA+4GT6xhTw00oFGsQ7ocwMyuqZhTTY0Dpg21IugD4RL2CarRCamJyDcLMbMBY3yh3fE2jaDL3QZiZlRtrglBNo2gy90GYmZUbtokpTc5XcRNtliCKNYgtve6DMDMr2lofxEogqJwMNtcnnOZwH4SZWblhE0REzGtkIM3UWXAfhJnZUGPtg2grne6DMDMr4wTBQBOTXztqZjbACYKBTmrXIMzMBlSVICQdIumUtDxTUlX9E5KOkrRK0mpJZ1fYfrKkbkn3pM/flWw7SdLD6XNStRc0FgU/SW1mVmbEJ6klnQcsBF5PNovrBOC7ZO+J3tpxBeAi4HBgLXC3pOUVXh16bUScMeTY6UDxewNYmY59rqqrGiU/KGdmVq6aGsR7yKba2AgQEX8AdqjiuAOA1RGxJiI2A9cAx1UZ15HALRHxbEoKtwBHVXnsqLkPwsysXDUJYnNEBNn/5JE0pcpz7wY8UbK+NpUN9T5J90paJmnOKI+tiQkFj2IyMxuqmgRxnaRLgJ0k/T1wK3Bpjb7/BmBuRLyJrJZwxWgOlrREUpekru7u7jEH0V+DcB+EmVm/ERNERFwALAO+T9YPcW5EfL2Kc68D5pSsz05lped+JiI2pdVvAm+u9th0/NKIWBgRC2fOHPs7jDyKycys3Iid1AARcQvZ//BH425gQRrxtA5YBPxN6Q6SZkXEk2n1WODBtLwC+KKkaWn9COCcUX5/1QrupDYzK1PNKKYXSf0PJTYAXcDHI2JNpeMiokfSGWR/7AvAZRFxv6Tzga6IWA6cKelYoAd4lvQiooh4VtI/kyUZgPMj4tlRX12Vik9Su5PazGxANTWIC8k6ib9HNnHfImA+8CvgMuDtwx0YETcBNw0pO7dk+RyGqRlExGXp/HU3sTNLEJt7exvxdWZmLaGaTupjI+KSiHgxIl6IiKXAkRFxLTBtpINbQX+C6HEntZlZUTUJ4mVJx0vqSJ/jgVfTtrZok5lYcIIwMxuqmgTxAeBvgfXAH9PyiZK2A87Y2oGtoliD2OQEYWbWb8Q+iNQJ/e5hNv93bcNpjklOEGZmZaoZxTQZOA14IzC5WB4Rp9YxroZyE5OZWblqmpiuBF5LNj/SnWQPrb1Yz6AaraNDTCiIzX4ntZlZv2oSxF4R8VlgY0RcAbwLOLC+YTXexEKHaxBmZiWqSRBb0s/nJe0D7AjsUr+QmmNipxOEmVmpah6UW5qmvPgMsByYCny2rlE1wcTODjb1+EE5M7OirSYISR3AC+mdDD8F9mxIVE0wqbPgGoSZWYmtNjFFRB/wyQbF0lQTOzvcSW1mVqKaPohbJX1C0hxJ04ufukfWYO6kNjMbrJo+iBPSz9NLyoI2a27K+iCcIMzMiqp5knpeIwJpNo9iMjMbbMQmJknbS/qMpKVpfYGkY+ofWmNNcg3CzGyQavogLgc2A29N6+uAz9ctoiZxH4SZ2WDVJIj5EfFl0gNzEfEy2YuD2sqkCR7FZGZWqpoEsTlN7R0AkuYDm6o5uaSjJK2StFrS2RW2/y9JD0i6V9JtkvYo2dYr6Z70WV7l9YyZaxBmZoNVM4rpc8DNwBxJVwFvI707emskFYCLgMPJXll6t6TlEfFAyW6/BhZGxMuSPgx8mYFRU69ExL7VXkhe7qQ2MxusmlFMP5a0EjiIrGnprIh4uopzHwCsTu+TQNI1wHFAf4KIiDtK9v8FcOIoYq8pPyhnZjZYNaOYbgCOAH4SETdWmRwAdgOeKFlfm8qGcxrwnyXrkyV1SfqFpL8aJrYlaZ+u7u7uKsOqbHJngVc2ey4mM7OiavogLgD+AnhA0jJJ708vEaoZSScCC4F/LSneIyIWAn8DXJj6PgaJiKURsTAiFs6cOTNXDFMnd/LKll56+9riNdtmZrmNmCAi4s6I+AeyJ6cvAY4nez/1SNYBc0rWZ6eyQSS9A/g0cGxE9Hd+R8S69HMN8BNgvyq+c8ymTspa217a1FPPrzEzaxnV1CBIo5jeB3wIeAtwRRWH3Q0skDRP0kRgEdl04aXn3Y8s6RwbEetLyqdJmpSWZ5B1jJd2btfcDpOdIMzMSlXzTurryDqcbwa+AdyZZnndqojokXQGsAIoAJdFxP2Szge6ImI5WZPSVOB6SQCPR8SxwJ8Cl0jqI0ti/zJk9FPNTZ00AYCXXnWCMDOD6oa5fgtYHBG9AJIOkbQ4Ik4f4Tgi4ibgpiFl55Ysv2OY434G/FkVsdXMQA1iywh7mpmND9UMc10haT9Ji8n6Hx4FflD3yBpsakoQL7oGYWYGbCVBSHodsDh9ngauBRQRf9mg2BpqB3dSm5kNsrUaxEPAfwHHRMRqAEkfa0hUTVCsQbgPwswss7VRTO8FngTukHSppMNow0n6ijzM1cxssGETRET8MCIWAW8A7gA+Cuwi6WJJRzQqwEaZMtF9EGZmpap5UG5jRHwvIt5N9rDbr4FP1T2yBuvoEK+Z3MlzL29udihmZtuEqh6UK4qI59L0FofVK6BmmjN9ex5/9uVmh2Fmtk0YVYJod/NmTOG+dRvY8LKfhTAzc4Io8YED9+D5l7dwzDf+i/vWbWh2OGZmTeUEUeLg+Ttz7f88mJ7e4K///eesuP+pZodkZtY0ThBDvHmPafzojLfxutfuwIe+u5KlP32ECE8BbmbjjxNEBbvsMJlrlxzE0fvM4os3PcSHv/srul+s6jXcZmZtwwliGJMnFPj64v04+51v4PZV6zn0gp/wlVt+5w5sMxs31C7NJwsXLoyurq66nPuR7pf415tXcfP9TzGps4Mj3/hajtrntRw4bzo7T51Ul+80M2sESSvT2zvLtzlBVO+BP7zA1b98nOW/+QMbXslqEvNnTuF1u+7A/JlTmb/LFHZ9zWRmTJ3EzlMmstP2Eyl0tO3sJGbWBpwgamxLbx+/XbeBX6x5hl899jxrul/isWdfLnufdYdgh8kTmDKxwPaTOrOfEzuZMqnA5AkFOjtEZ6Ej/RSdHdlyoSAmdHTQn1skShYRQqlAqSzbNpCMtrpf+06p1XLkW2E1MHOHSRy3725jOnZrCaKaFwbZEBMKHey/+zT2331af9nmnj4ef/Zlul/cxDMbN/H0i5t4ZuNmXny1h42benh5cy8bN/fw8qZentzwKq9s7qWnL+jp7aOnL+jtC7b09mU/U3kAbZK/zayO9p2z05gTxNbUNUFIOgr4N7JXjn4zIv5lyPZJwHeANwPPACdExO/TtnOA04Be4MyIWFHPWPOa2NnBXrtMZa9dptbtOyKCCIjS9f5lKK4Vk0r/T6Jk2bYV7VJ7bxetfDcKdaqK1i1BSCoAFwGHA2uBuyUtH/Ju6dOA5yJiL0mLgC8BJ0jaG1gEvBH4E+BWSa8rvvZ0vJI0pEnC7RNmVj/1HOZ6ALA6ItZExGbgGuC4IfscB1yRlpcBhylrSD8OuCYiNkXEo8DqdD4zM2uQeiaI3YAnStbXprKK+0RED7AB2LnKY5G0RFKXpK7u7u4ahm5mZi3dSR0RS4GlAJK6JT2W43QzyN69PZ74mtvfeLte8DWP1h7DbahnglgHzClZn53KKu2zVlInsCNZZ3U1xw4SETPzBCupa7ihXu3K19z+xtv1gq+5lurZxHQ3sEDSPEkTyTqdlw/ZZzlwUlp+P3B7ZEM7lgOLJE2SNA9YAPyyjrGamdkQdatBRESPpDOAFWTDXC+LiPslnQ90RcRy4FvAlZJWA8+SJRHSftcBDwA9wOnjfQSTmVmj1bUPIiJuAm4aUnZuyfKrwF8Pc+wXgC/UM74hljbwu7YVvub2N96uF3zNNdM2U22YmVltebpvMzOryAnCzMwqGvcJQtJRklZJWi3p7GbHUyuS5ki6Q9IDku6XdFYqny7pFkkPp5/TUrkkfS39Hu6VtH9zr2DsJBUk/VrSjWl9nqS70rVdm0bVkUbJXZvK75I0t5lxj5WknSQtk/SQpAclHdzu91nSx9K/6/skXS1pcrvdZ0mXSVov6b6SslHfV0knpf0flnRSpe8azrhOECXzRb0T2BtYnOaBagc9wMcjYm/gIOD0dG1nA7dFxALgtrQO2e9gQfosAS5ufMg1cxbwYMn6l4CvRsRewHNkc4BByVxgwFfTfq3o34CbI7Ib9WsAAARPSURBVOINwJ+TXXvb3mdJuwFnAgsjYh+yUZLFudza6T5/GzhqSNmo7quk6cB5wIFk0xWdV0wqVclmCB2fH+BgYEXJ+jnAOc2Oq07X+iOyiRNXAbNS2SxgVVq+BFhcsn//fq30IXuo8jbgUOBGshkNnwY6h95zsiHYB6flzrSfmn0No7zeHYFHh8bdzveZgal4pqf7diNwZDveZ2AucN9Y7yuwGLikpHzQfiN9xnUNgirnfGp1qUq9H3AXsGtEPJk2PQXsmpbb5XdxIfBJoC+t7ww8H9lcXzD4uoabC6yVzAO6gctTs9o3JU2hje9zRKwDLgAeB54ku28rae/7XDTa+5rrfo/3BNH2JE0Fvg98NCJeKN0W2X8p2macs6RjgPURsbLZsTRQJ7A/cHFE7AdsZKDZAWjL+zyNbMbneWSvA5hCeVNM22vEfR3vCWLUcz61EkkTyJLDVRHxg1T8R0mz0vZZwPpU3g6/i7cBx0r6Pdn08oeStc/vlOb6gsHX1X/NQ+YCayVrgbURcVdaX0aWMNr5Pr8DeDQiuiNiC/ADsnvfzve5aLT3Ndf9Hu8Jopr5olqSJJFNZfJgRHylZFPp/FcnkfVNFMs/mEZDHARsKKnKtoSIOCciZkfEXLJ7eXtEfAC4g2yuLyi/5kpzgbWMiHgKeELS61PRYWRT1LTtfSZrWjpI0vbp33nxmtv2PpcY7X1dARwhaVqqeR2RyqrT7E6YZn+Ao4HfAY8An252PDW8rkPIqp/3Avekz9Fkba+3AQ8DtwLT0/4iG9H1CPBbshEiTb+OHNf/duDGtLwn2WSPq4HrgUmpfHJaX52279nsuMd4rfsCXele/xCY1u73Gfgn4CHgPuBKYFK73WfgarI+li1kNcXTxnJfgVPTta8GThlNDJ5qw8zMKhrvTUxmZjYMJwgzM6vICcLMzCpygjAzs4qcIMzMrCInCLNRkNQr6Z6ST81mAJY0t3TmTrNmq+srR83a0CsRsW+zgzBrBNcgzGpA0u8lfVnSbyX9UtJeqXyupNvTHP23Sdo9le8q6T8k/SZ93ppOVZB0aXrXwY8lbde0i7JxzwnCbHS2G9LEdELJtg0R8WfAN8hmlQX4OnBFRLwJuAr4Wir/GnBnRPw52dxJ96fyBcBFEfFG4HngfXW+HrNh+Ulqs1GQ9FJETK1Q/nvg0IhYkyZJfCoidpb0NNn8/VtS+ZMRMUNSNzA7IjaVnGMucEtkL4NB0qeACRHx+fpfmVk51yDMaieGWR6NTSXLvbif0JrICcKsdk4o+fnztPwzspllAT4A/Fdavg34MPS/Q3vHRgVpVi3/78RsdLaTdE/J+s0RURzqOk3SvWS1gMWp7CNkb3v7R7I3v52Sys8Clko6jaym8GGymTvNthnugzCrgdQHsTAinm52LGa14iYmMzOryDUIMzOryDUIMzOryAnCzMwqcoIwM7OKnCDMzKwiJwgzM6vo/wONvp/jVeNHhQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing one test:\n",
            "Output for input 0.4319341932813665: [[0.93200326]]\n",
            "Output for input 0.23427407534964595: [[0.95104711]]\n",
            "Output for input 0.6480704786654603: [[0.9111793]]\n",
            "Output for input 0.1239756747037094: [[0.96167396]]\n",
            "Output for input 0.045737147527478705: [[0.96904541]]\n",
            "Output for input 0.7093287599751554: [[0.90527728]]\n",
            "Output for input 0.5525036825086314: [[0.92038682]]\n",
            "Output for input 0.8776523798071523: [[0.8890599]]\n",
            "Output for input 0.02310238834506939: [[0.96106682]]\n",
            "Output for input 0.27345684109019963: [[0.94727199]]\n"
          ]
        }
      ]
    }
  ]
}