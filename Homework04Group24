{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliBenovaa/IANNwTF_Group24/blob/main/Homework04Group24\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JCY98nVht9x",
        "outputId": "35806b4f-fb78-4cf6-a508-5f6d69325363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 784) (32, 784) (32,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tqdm \n",
        "import matplotlib as plt\n",
        "\n",
        "\n",
        "# 1. get mnist from tensorflow_datasets\n",
        "mnist = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
        "\n",
        "train_ds = mnist[0]\n",
        "val_ds = mnist[1]\n",
        "\n",
        "train_ds2 =mnist[0]\n",
        "val_ds2 = mnist[1]\n",
        "\n",
        "\n",
        "# 2. write function to create the dataset that we want\n",
        "def preprocess(data, batch_size,condition):\n",
        "    # image should be float\n",
        "    data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
        "    # image should be flattened\n",
        "    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
        "    # image vector will here have values between -1 and 1\n",
        "    data = data.map(lambda x,t: ((x/128.)-1., t))\n",
        "    # we want to have two mnist images in each example\n",
        "    # this leads to a single example being ((x1,y1),(x2,y2))\n",
        "    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), \n",
        "                                     data.shuffle(2000)))\n",
        "    \n",
        "    if (condition == \"bigger_5\"):\n",
        "        # map ((x1,y1),(x2,y2)) to (x1,x2, y1==y2*) *boolean\n",
        "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
        "        # transform boolean target to int\n",
        "        zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
        "    elif (condition==\"minus\"):\n",
        "        # map ((x1,y1),(x2,y2)) to (x1,x2, y1==y2*) *boolean\n",
        "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1]))\n",
        "        # transform boolean target to int\n",
        "        zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.float32)))\n",
        "    \n",
        "    # batch the dataset\n",
        "    zipped_ds = zipped_ds.batch(batch_size)\n",
        "    # prefetch\n",
        "    zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return zipped_ds\n",
        "\n",
        "train_ds = preprocess(train_ds, batch_size=32,condition = \"bigger_5\") #train_ds.apply(preprocess)\n",
        "val_ds = preprocess(val_ds, batch_size=32, condition=\"bigger_5\" ) #val_ds.apply(preprocess)\n",
        "\n",
        "train_ds2 = preprocess(train_ds2, batch_size=32,condition = \"minus\") #train_ds.apply(preprocess)\n",
        "val_ds2 = preprocess(val_ds2, batch_size=32, condition=\"minus\" ) #val_ds.apply(preprocess)\n",
        "\n",
        "\n",
        "#checking\n",
        "for img1, img2, label in train_ds.take(1):\n",
        "    print(img1.shape, img2.shape, label.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwinMNISTModel(tf.keras.Model):\n",
        "\n",
        "    # 1. constructor\n",
        "    def __init__(self,optimizer,loss_function,train_ds,test_ds):\n",
        "        super().__init__()\n",
        "        # inherit functionality from parent class\n",
        "\n",
        "        # optimizer, loss function and metrics\n",
        "        self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),\n",
        "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        self.loss_function = loss_function\n",
        "\n",
        "      \n",
        "        \n",
        "        # layers to be used\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        \n",
        "        self.out_layer = tf.keras.layers.Dense(1,activation=tf.nn.sigmoid)\n",
        "\n",
        "        \n",
        "        \n",
        "    # 2. call method (forward computation)\n",
        "    def call(self, images, training=False):\n",
        "        img1, img2 = images\n",
        "        \n",
        "        img1_x = self.dense1(img1)\n",
        "        img1_x = self.dense2(img1_x)\n",
        "        \n",
        "        img2_x = self.dense1(img2)\n",
        "        img2_x = self.dense2(img2_x)\n",
        "        \n",
        "        combined_x = tf.concat([img1_x, img2_x ], axis=1)\n",
        "        \n",
        "        return self.out_layer(combined_x)\n",
        "\n",
        "    \n",
        "\n",
        "    # 3. metrics property\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "        # return a list with all metrics in the model\n",
        "\n",
        "\n",
        "\n",
        "    # 4. reset all metrics objects\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        img1, img2, label = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self((img1, img2), training=True)\n",
        "            loss = self.loss_function(label, output)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        \n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update the state of the metrics according to loss\n",
        "        self.metrics[0].update_state(label, output)\n",
        "        self.metrics[1].update_state(loss)\n",
        "        \n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "   # 6. test_step method\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        img1, img2, label = data\n",
        "        # same as train step (without parameter updates)\n",
        "        output = self((img1, img2), training=False)\n",
        "        loss = self.loss_function(label, output)\n",
        "        self.metrics[0].update_state(label, output)\n",
        "        self.metrics[1].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "d67GJivZln4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc=[]\n",
        "test_acc=[]\n",
        "\n",
        "def training_loop(subtask,train_ds, test_ds):\n",
        "\n",
        "  #items for visualization\n",
        "  train = np.empty(0)\n",
        "  test = np.empty(0)\n",
        "  #train_acc = np.empty(0)\n",
        "  #test_acc = np.empty(0)\n",
        "    \n",
        "  if subtask == \"minus\":\n",
        "    model  = TwinMNISTModel(tf.keras.losses.MeanSquaredError(), tf.keras.optimizers.Adam,train_ds,test_ds)\n",
        "\n",
        "  elif subtask == \"bigger_5\":\n",
        "    model = TwinMNISTModel(tf.keras.losses.BinaryCrossentropy(from_logits = True), tf.keras.optimizers.Adam,train_ds,test_ds)\n",
        "\n",
        "  for epoch in range(2):\n",
        "      print(f\"Epoch {epoch}:\")\n",
        "      \n",
        "      # Training:\n",
        "      \n",
        "      for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "          metrics = model.train_step(data)\n",
        "          \n",
        "    \n",
        "          for metric in model.metrics:\n",
        "              tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "      # print the metrics\n",
        "      print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "      #experiments: save into array for later \n",
        "      train = np.append(train,metrics[\"loss\"])\n",
        "      #train_acc = np.append(train_acc,metrics[\"acc\"])\n",
        "      \n",
        "\n",
        "      # reset all metrics (requires a reset_metrics method in the model)\n",
        "      model.reset_metrics()    \n",
        "      \n",
        "      # Testing:\n",
        "      for data in test_ds:\n",
        "          metrics = model.test_step(data)\n",
        "  \n",
        "          for metric in model.metrics:\n",
        "              tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "                  \n",
        "      print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "      #experiments: save into array for later \n",
        "      test = np.append(test,metrics[\"loss\"])\n",
        "      test_acc = np.append(test_acc,metrics[\"acc\"])\n",
        "\n",
        "      # reset all metrics\n",
        "      model.reset_metrics()\n",
        "      print(\"\\n\")\n",
        "  return train,test ,train_acc, test_acc"
      ],
      "metadata": {
        "id": "sOHe9ABqv_34"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. pass arguments to training loop function\n",
        "training_loop(\"minus\",train_ds,val_ds)\n",
        "training_loop(\"bigger_5\",train_ds2,val_ds2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6oBS9JbsLuB",
        "outputId": "8158bdd9-08b8-470a-e9d4-be0ab601f6d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:15<00:00, 120.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['binary_accuracy: 0.9157833456993103', 'loss: 0.21196751296520233']\n",
            "['test_binary_accuracy: 0.9327999949455261', 'test_loss: 0.16274866461753845']\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:20<00:00, 91.55it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['binary_accuracy: 0.9428166747093201', 'loss: 0.1463342159986496']\n",
            "['test_binary_accuracy: 0.9524000287055969', 'test_loss: 0.12467912584543228']\n",
            "\n",
            "\n",
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:20<00:00, 91.54it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['binary_accuracy: 0.11299999803304672', 'loss: -33.464027404785156']\n",
            "['test_binary_accuracy: 0.11760000139474869', 'test_loss: -38.422996520996094']\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:20<00:00, 91.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['binary_accuracy: 0.12251666933298111', 'loss: -38.60445022583008']\n",
            "['test_binary_accuracy: 0.12919999659061432', 'test_loss: -39.62358474731445']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-33.4640274 , -38.60445023]), array([-38.42299652, -39.62358475]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}