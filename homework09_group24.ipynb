{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliBenovaa/IANNwTF_Group24/blob/main/homework09_group24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI76e8GaK_5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f5c725-2f00-4371-d961-880d7b6df926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "#from tensorflow.keras import layers\n",
        "import time\n",
        "import urllib\n",
        "from IPython import display\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tqdm \n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckynsyFZFJvW"
      },
      "outputs": [],
      "source": [
        "def download_data():\n",
        "    category = 'candle'\n",
        "    if not os.path.isfile(f\"npy_files/{category}.npy\"):\n",
        "        print(\"Start downloading data...\")\n",
        "        #Download files\n",
        "        # categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
        "        # print(categories[:10])\n",
        "\n",
        "        # Creates a folder to download the original drawings into.\n",
        "        # We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
        "\n",
        "        if not os.path.isdir('npy_files'):\n",
        "            os.mkdir('npy_files')\n",
        "\n",
        "        url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'\n",
        "        urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
        "        print(\"Finished downloading data!\")\n",
        "    else:\n",
        "        print(\"Data was already downloaded. Proceeding...\")\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    # cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "    dataset = dataset.cache()\n",
        "    # convert data from uint8 to float32\n",
        "    dataset = dataset.map(lambda vector: tf.cast(vector, tf.float32))\n",
        "    # sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "    dataset = dataset.map(lambda vector: (vector / 128.) - 1.)\n",
        "    # reshape tensor\n",
        "    dataset = dataset.map(lambda vector: tf.reshape(vector, (28, 28)))\n",
        "    # expand dim\n",
        "    dataset = dataset.map(lambda vector: tf.expand_dims(vector, axis=-1))\n",
        "    # shuffle, batch, prefetch\n",
        "    dataset = dataset.shuffle(50000)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(32)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtL8gVqQFXDK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "NOISE_INPUT_DIM = 100\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLOvsJNFFY-V"
      },
      "outputs": [],
      "source": [
        "class Generator(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class represents Generator of the GAN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # self.net_layers = []\n",
        "        # # initial Dense-Layer\n",
        "        # self.net_layers.append(tf.keras.layers.Dense(7*7*128, use_bias=True, input_shape=(input_dim,)))\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Reshape((7,7,128)))\n",
        "        # # use upsampling\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=True)) # hints state that Conv2DTrans might work better with even kernel-size\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=True)) # hints state that Conv2DTrans might work better with even kernel-size\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # # last convlayer with tanh\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2D(1, (3, 3), strides=(1, 1), padding='same', use_bias=True, activation='tanh'))\n",
        "        \n",
        "        self.net_layers = []\n",
        "        # initial Dense-Layer\n",
        "        self.net_layers.append(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(input_dim,)))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        self.net_layers.append(tf.keras.layers.Reshape((7,7,256)))\n",
        "        # use upsampling\n",
        "        self.net_layers.append(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # last convlayer with tanh\n",
        "        self.net_layers.append(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        self.loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        # tf.keras.metrics.BinaryAccuracy, \n",
        "                        ]\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        calculates the output of the network for\n",
        "        the given input\n",
        "        :param inputs: the input tensor of the network\n",
        "        :training: boolean for training\n",
        "        :return: output of the network\n",
        "        \"\"\"\n",
        "        for layer in self.net_layers:\n",
        "            inputs = layer(inputs, training=training)\n",
        "        return inputs\n",
        "\n",
        "    def reset_metrics(self):    \n",
        "        for metric in self.metrics_list:\n",
        "            metric.reset_states()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, noise, discriminator):\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self(noise, training=True)\n",
        "            fake_output = discriminator(generated_images, training=False)\n",
        "            fake_targets = tf.ones_like(fake_output)\n",
        "\n",
        "            total_loss = self.loss(fake_targets, fake_output)\n",
        "        \n",
        "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics_list[0].update_state(total_loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics_list[1:]:\n",
        "            metric.update_state(y_true=fake_targets, y_pred=fake_output)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return ({m.name: m.result() for m in self.metrics_list}, generated_images)\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, noise, discriminator):\n",
        "\n",
        "        generated_images = self(noise, training=False)\n",
        "        fake_output = discriminator(generated_images, training=False)\n",
        "        fake_targets = tf.ones_like(fake_output)\n",
        "\n",
        "        total_loss = self.loss(fake_targets, fake_output)\n",
        "        \n",
        "        self.metrics_list[0].update_state(total_loss)\n",
        "        \n",
        "        for metric in self.metrics_list[1:]:\n",
        "            metric.update_state(y_true=fake_targets, y_pred=fake_output)\n",
        "\n",
        "        return ({m.name: m.result() for m in self.metrics_list}, generated_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4acgyj6Fc8L"
      },
      "outputs": [],
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This class represents Discriminator of the GAN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # self.net_layers = []\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='valid',\n",
        "        #                                              input_shape=[28, 28, 1]))\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Dropout(0.2))\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Dropout(0.2))\n",
        "        # self.net_layers.append(tf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
        "        # self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        # self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        # self.net_layers.append(tf.keras.layers.Dropout(0.2))\n",
        "        # self.net_layers.append(tf.keras.layers.Flatten())\n",
        "        # # Dense classification layer\n",
        "        # self.net_layers.append(tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid))\n",
        "\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                                     input_shape=[28, 28, 1]))\n",
        "        self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        self.net_layers.append(tf.keras.layers.Dropout(0.3))\n",
        "        self.net_layers.append(\n",
        "            tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.LeakyReLU())\n",
        "        self.net_layers.append(tf.keras.layers.Dropout(0.3))\n",
        "        self.net_layers.append(tf.keras.layers.Flatten())\n",
        "        # Dense classification layer\n",
        "        self.net_layers.append(tf.keras.layers.Dense(1))\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        self.loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        # tf.keras.metrics.BinaryAccuracy, \n",
        "                        ]\n",
        "\n",
        "    # should return values near1 for real images and 0 for fake images\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        calculates the output of the network for\n",
        "        the given input\n",
        "        :param inputs: the input tensor of the network\n",
        "        :training: boolean for training\n",
        "        :return: output of the network\n",
        "        \"\"\"\n",
        "        for layer in self.net_layers:\n",
        "            inputs = layer(inputs, training=training)\n",
        "        return inputs\n",
        "\n",
        "    def reset_metrics(self):    \n",
        "        for metric in self.metrics_list:\n",
        "            metric.reset_states()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, real_images, generated_images):\n",
        "        with tf.GradientTape() as tape:\n",
        "            real_output = self(real_images, training=True)\n",
        "            fake_output = self(generated_images, training=True)\n",
        "            real_targets = tf.ones_like(real_output)\n",
        "            fake_targets = tf.zeros_like(fake_output)\n",
        "            \n",
        "            real_loss = self.loss(real_targets, real_output)\n",
        "            fake_loss = self.loss(fake_targets, fake_output)\n",
        "            total_loss = real_loss + fake_loss\n",
        "        \n",
        "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics_list[0].update_state(total_loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics_list[1:]:\n",
        "            metric.update_state(y_true=real_targets, y_pred=real_output)\n",
        "            metric.update_state(y_true=fake_targets, y_pred=fake_output)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics_list}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, real_images, generated_images):\n",
        "\n",
        "        real_output = self(real_images, training=False)\n",
        "        fake_output = self(generated_images, training=False)\n",
        "        real_targets = tf.ones_like(real_output)\n",
        "        fake_targets = tf.zeros_like(fake_output)\n",
        "        \n",
        "        real_loss = self.loss(real_targets, real_output)\n",
        "        fake_loss = self.loss(fake_targets, fake_output)\n",
        "        total_loss = real_loss + fake_loss\n",
        "                \n",
        "        self.metrics_list[0].update_state(total_loss)\n",
        "        \n",
        "        for metric in self.metrics_list[1:]:\n",
        "            metric.update_state(y_true=real_targets, y_pred=real_output)\n",
        "            metric.update_state(y_true=fake_targets, y_pred=fake_output)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uhRdORTvFxh5",
        "outputId": "6587f87c-0ea3-41fa-b23a-a05bfbb6ae44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading data...\n",
            "Finished downloading data!\n",
            "Total num of images: 141545\n",
            "Shape of dataset: <PrefetchDataset element_spec=TensorSpec(shape=(256, 28, 28, 1), dtype=tf.float32, name=None)>\n",
            "Time for epoch 1 is 34.938507318496704 sec\n",
            "['gen_loss: 0.645923912525177']\n",
            "['disc_loss: 1.3127732276916504']\n",
            "['gen_val_loss: 0.7062961459159851']\n",
            "['disc_val_loss: 1.3812612295150757']\n",
            "\n",
            "\n",
            "Time for epoch 2 is 24.971559524536133 sec\n",
            "['gen_loss: 0.7303762435913086']\n",
            "['disc_loss: 1.344249963760376']\n",
            "['gen_val_loss: 0.6987409591674805']\n",
            "['disc_val_loss: 1.326880693435669']\n",
            "\n",
            "\n",
            "Time for epoch 3 is 25.283947944641113 sec\n",
            "['gen_loss: 0.7428883910179138']\n",
            "['disc_loss: 1.3532174825668335']\n",
            "['gen_val_loss: 0.7482272982597351']\n",
            "['disc_val_loss: 1.3730212450027466']\n",
            "\n",
            "\n",
            "Time for epoch 4 is 25.700069189071655 sec\n",
            "['gen_loss: 0.8289933204650879']\n",
            "['disc_loss: 1.2622591257095337']\n",
            "['gen_val_loss: 1.2439055442810059']\n",
            "['disc_val_loss: 0.8388450741767883']\n",
            "\n",
            "\n",
            "Time for epoch 5 is 25.828323364257812 sec\n",
            "['gen_loss: 1.0247617959976196']\n",
            "['disc_loss: 1.1519442796707153']\n",
            "['gen_val_loss: 1.1891448497772217']\n",
            "['disc_val_loss: 1.0359041690826416']\n",
            "Picture from epoch 4 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Time for epoch 6 is 25.986674070358276 sec\n",
            "['gen_loss: 0.9287428855895996']\n",
            "['disc_loss: 1.2251321077346802']\n",
            "['gen_val_loss: 1.2437769174575806']\n",
            "['disc_val_loss: 1.0596123933792114']\n",
            "\n",
            "\n",
            "Time for epoch 7 is 26.177000522613525 sec\n",
            "['gen_loss: 1.0548135042190552']\n",
            "['disc_loss: 1.1311266422271729']\n",
            "['gen_val_loss: 0.9447519183158875']\n",
            "['disc_val_loss: 1.1540073156356812']\n",
            "\n",
            "\n",
            "Time for epoch 8 is 26.24488615989685 sec\n",
            "['gen_loss: 1.0496301651000977']\n",
            "['disc_loss: 1.1395164728164673']\n",
            "['gen_val_loss: 1.1888595819473267']\n",
            "['disc_val_loss: 0.987143874168396']\n",
            "\n",
            "\n",
            "Time for epoch 9 is 26.386345863342285 sec\n",
            "['gen_loss: 1.06581449508667']\n",
            "['disc_loss: 1.0829428434371948']\n",
            "['gen_val_loss: 1.0493329763412476']\n",
            "['disc_val_loss: 1.2018431425094604']\n",
            "\n",
            "\n",
            "Time for epoch 10 is 26.14513397216797 sec\n",
            "['gen_loss: 1.0886362791061401']\n",
            "['disc_loss: 1.104262351989746']\n",
            "['gen_val_loss: 1.139290690422058']\n",
            "['disc_val_loss: 1.0859549045562744']\n",
            "Picture from epoch 9 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define where to save the log\n",
        "hyperparameter_string= \"Your_Settings_Here\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"test_logs/{hyperparameter_string}/{current_time}/train\"\n",
        "val_log_path = f\"test_logs/{hyperparameter_string}/{current_time}/val\"\n",
        "train_log_path = \"test_logs/train\"\n",
        "val_log_path =  \"test_logs/val\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "\n",
        "\n",
        "# first download data if not done before\n",
        "download_data()\n",
        "category = 'candle'\n",
        "# # images of candels 141545\n",
        "# loading the dataset and split into train and test set\n",
        "images = np.load(f'npy_files/{category}.npy')\n",
        "print(f\"Total num of images: {len(images)}\")\n",
        "train_images = images[:100000]\n",
        "test_images = images[100000:140000]\n",
        "\n",
        "# maybe a smaller dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images))\n",
        "prepared_train_dataset = train_dataset.apply(prepare_dataset)\n",
        "prepared_test_dataset = test_dataset.apply(prepare_dataset)\n",
        "print(f\"Shape of dataset: {prepared_test_dataset}\")\n",
        "\n",
        "generator = Generator(input_dim=NOISE_INPUT_DIM)\n",
        "discriminator = Discriminator()\n",
        "test_image_noise = tf.random.normal([1, NOISE_INPUT_DIM])\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Training:\n",
        "    for image_batch in prepared_train_dataset:\n",
        "        noise = tf.random.normal([BATCH_SIZE, NOISE_INPUT_DIM])\n",
        "        gen_metrics, generated_images = generator.train_step(noise, discriminator)\n",
        "        disc_metrics =  discriminator.train_step(image_batch, generated_images)\n",
        "\n",
        "    # print the metrics\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n",
        "    print([f\"gen_{key}: {value}\" for (key, value) in zip(list(gen_metrics.keys()), list(gen_metrics.values()))])\n",
        "    print([f\"disc_{key}: {value}\" for (key, value) in zip(list(disc_metrics.keys()), list(disc_metrics.values()))])\n",
        "    \n",
        "    # logging the training metrics to the log file which is used by tensorboard\n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in generator.metrics:\n",
        "            tf.summary.scalar(f\"gen_{metric.name}\", metric.result(), step=epoch)\n",
        "        for metric in discriminator.metrics:\n",
        "            tf.summary.scalar(f\"disc_{metric.name}\", metric.result(), step=epoch)\n",
        "    \n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    generator.reset_metrics()\n",
        "    discriminator.reset_metrics()\n",
        "\n",
        "    # Validation:\n",
        "\n",
        "    for data in prepared_test_dataset:\n",
        "        noise = tf.random.normal([BATCH_SIZE, NOISE_INPUT_DIM])\n",
        "        gen_metrics, generated_images = generator.test_step(noise, discriminator)\n",
        "        disc_metrics =  discriminator.test_step(image_batch, generated_images)\n",
        "    \n",
        "    print([f\"gen_val_{key}: {value}\" for (key, value) in zip(list(gen_metrics.keys()), list(gen_metrics.values()))])\n",
        "    print([f\"disc_val_{key}: {value}\" for (key, value) in zip(list(disc_metrics.keys()), list(disc_metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with val_summary_writer.as_default():\n",
        "        for metric in generator.metrics:\n",
        "            tf.summary.scalar(f\"gen_val_{metric.name}\", metric.result(), step=epoch)\n",
        "        for metric in discriminator.metrics:\n",
        "            tf.summary.scalar(f\"disc_val_{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # every (5) epochs generate images for the test_image_noise and write it to tensorboard\n",
        "        if (epoch+1)%5 == 0:\n",
        "            generated_test_images = generator(test_image_noise, training=False)\n",
        "            # save a batch of images for this epoch (have to be between 0 and 1)\n",
        "            tf.summary.image(name=\"generated_images\",data = generated_test_images, step=epoch, max_outputs=32)\n",
        "            \n",
        "            print(\"Picture from epoch {} \".format(epoch))\n",
        "            plt.imshow(generated_images[0, :, :, 0], cmap='gray')\n",
        "            plt.show()\n",
        "    \n",
        "    # reset all metrics\n",
        "    generator.reset_metrics()\n",
        "    discriminator.reset_metrics()   \n",
        "     \n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyyxjxQusaPr5D306yA3Dr",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}