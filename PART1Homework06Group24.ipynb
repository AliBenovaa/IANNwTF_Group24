{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliBenovaa/IANNwTF_Group24/blob/main/PART1Homework06Group24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w46cXuFr9R6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tqdm \n",
        "#import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "train_ds, test_ds =tfds.load(\"cifar10\", split =[\"train\",\"test\"], as_supervised=True)\n",
        "\n",
        "\n",
        "def prepare_cifar_data(cifar):\n",
        "  #convert data from uint64 to float32\n",
        "  cifar = cifar.map(lambda img, target: (tf.cast(img, tf.float32),target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [0, 1]\n",
        "  cifar = cifar.map(lambda img, target: ((img/128.)-1., target))\n",
        "  cifar = cifar.map(lambda img, target: (img, tf.cast(target,tf.int32)))\n",
        "  #create one-hot targets\n",
        "  cifar = cifar.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  cifar = cifar.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  cifar = cifar.shuffle(1000)\n",
        "  cifar = cifar.batch(16)\n",
        "  cifar = cifar.prefetch(20)\n",
        "  #return preprocessed dataset\n",
        "  return cifar\n",
        "\n",
        "train_ds = train_ds.apply(prepare_cifar_data)\n",
        "test_ds = test_ds.apply(prepare_cifar_data)\n",
        "\n",
        "def try_model(model, ds):\n",
        "  for x, t in ds.take(5):\n",
        "    y = model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO_BudYHD0yK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BasicConv(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "    \n",
        "        super(BasicConv, self).__init__()\n",
        "\n",
        "        # optimizer, loss function and metrics\n",
        "        self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "        \n",
        "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "\n",
        "        self.convlayer1 = tf.keras.layers.Conv2D(32,(3,3), padding='same', activation='relu', input_shape = (32,32,3))\n",
        "        self.pooling = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
        "\n",
        "        self.convlayer2 = tf.keras.layers.Conv2D(64,(3,3), padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l=0.01))\n",
        "        self.pooling2 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
        "        \n",
        "        self.convlayer3 = tf.keras.layers.Conv2D(128,(3,3), padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l=0.01))\n",
        "        self.pooling3 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
        "        \n",
        "        self.convlayer4 = tf.keras.layers.Conv2D(256,(3,3), padding='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l=0.01))\n",
        "\n",
        "        self.global_pool = tf.keras.layers.MaxPool2D((2,2))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense1 = tf.keras.layers.Dense(64,activation = 'relu')\n",
        "        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "        #self.dropout_layer = tf.keras.layers.Dropout(rate=0.2)\n",
        "\n",
        "    def call(self,inputs):\n",
        "\n",
        "        x = self.convlayer1(inputs)\n",
        "        x = self.pooling(x)\n",
        "        x = self.convlayer2(x)\n",
        "        x = self.pooling2(x)\n",
        "        x = self.convlayer3(x)\n",
        "        x = self.pooling3(x)\n",
        "        x = self.convlayer4(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.out(x)\n",
        "        return  x\n",
        "\n",
        "\n",
        "\n",
        "     # 3. metrics property\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "        # return a list with all metrics in the model\n",
        "\n",
        "\n",
        "\n",
        "    # 4. reset all metrics objects\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "\n",
        "     #train_step method\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        img1, label = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(img1, training=True)\n",
        "            loss = self.loss_function(label, output) + self.losses\n",
        "    \n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        \n",
        "        self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "        \n",
        "        # update the state of the metrics according to loss\n",
        "        self.metrics[0].update_state(label, output)\n",
        "        self.metrics[1].update_state(loss)\n",
        "        \n",
        "        # return a dictionary with metric names as keys and metric results as values\n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "   # 6. test_step method\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        img1, label = data\n",
        "        # same as train step (without parameter updates)\n",
        "        output = self(img1, training=False)\n",
        "        loss = self.loss_function(label, output)\n",
        "        self.metrics[0].update_state(label, output)\n",
        "        self.metrics[1].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnNf8ElHYxHL"
      },
      "outputs": [],
      "source": [
        "class ResidualConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters):\n",
        "    super(ResidualConnectedCNNLayer, self).__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    c = self.conv(x)\n",
        "    x = c+x\n",
        "    return x\n",
        "\n",
        "class ResidualConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth, layers):\n",
        "    super(ResidualConnectedCNNBlock, self).__init__()\n",
        "    self.deeper_layer = tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu')\n",
        "    self.layers = [ResidualConnectedCNNLayer(depth) for _ in range(layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.deeper_layer(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "class ResidualConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ResidualConnectedCNN, self).__init__()\n",
        "\n",
        "\n",
        "    # optimizer, loss function and metrics  \n",
        "    self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "    self.optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate = 0.0001,momentum = 0.99,) #tf.keras.optimizers.Adam(learning_rate = 0.00001)\n",
        "        \n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "  \n",
        "    self.residualblock1 = ResidualConnectedCNNBlock(24,4)\n",
        "    self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "    self.residualblock2 = ResidualConnectedCNNBlock(48,4)\n",
        "    self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    self.residualblock3 = ResidualConnectedCNNBlock(96,4)\n",
        "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
        "    \n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax',activity_regularizer=tf.keras.regularizers.L2(0.01))\n",
        "\n",
        "\n",
        "  \n",
        "  def call(self,x):\n",
        "    x = self.residualblock1(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.residualblock2(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.residualblock3(x)\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "      # 3. metrics property\n",
        "  @property\n",
        "  def metrics(self):\n",
        "      return self.metrics_list\n",
        "      # return a list with all metrics in the model\n",
        "\n",
        "\n",
        "\n",
        "  # 4. reset all metrics objects\n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "          metric.reset_states()\n",
        "\n",
        "\n",
        "    #train_step method\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      img1, label = data\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "          output = self(img1, training=True)\n",
        "          loss = self.loss_function(label, output)\n",
        "  \n",
        "          \n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      \n",
        "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "      \n",
        "      # update the state of the metrics according to loss\n",
        "      self.metrics[0].update_state(label, output)\n",
        "      self.metrics[1].update_state(loss)\n",
        "      \n",
        "      # return a dictionary with metric names as keys and metric results as values\n",
        "      return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "  # 6. test_step method\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "      img1, label = data\n",
        "      # same as train step (without parameter updates)\n",
        "      output = self(img1, training=False)\n",
        "      loss = self.loss_function(label, output)\n",
        "      self.metrics[0].update_state(label, output)\n",
        "      self.metrics[1].update_state(loss)\n",
        "      \n",
        "      return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "resnet_model = ResidualConnectedCNN()\n",
        "try_model(resnet_model, train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgkVkgf3bkII"
      },
      "outputs": [],
      "source": [
        "class DenselyConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters):\n",
        "    super(DenselyConnectedCNNLayer, self).__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    c = self.conv(x)\n",
        "    x = tf.concat((x,c), axis=-1)\n",
        "    return x\n",
        "\n",
        "class DenselyConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, layers):\n",
        "    super(DenselyConnectedCNNBlock, self).__init__()\n",
        "    self.layers = [DenselyConnectedCNNLayer(num_filters) for _ in range(layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DenselyConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DenselyConnectedCNN, self).__init__()\n",
        "\n",
        "\n",
        "    # optimizer, loss function and metrics\n",
        "    self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                             tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "        \n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "\n",
        "\n",
        "    self.denseblock1 = DenselyConnectedCNNBlock(24,4)\n",
        "    self.pooling1 = tf.keras.layers.MaxPooling2D()\n",
        "\n",
        "    self.denseblock2 = DenselyConnectedCNNBlock(24,4)\n",
        "    self.pooling2 = tf.keras.layers.MaxPooling2D()\n",
        "\n",
        "    self.denseblock3 = DenselyConnectedCNNBlock(24,4)\n",
        "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.denseblock1(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.denseblock2(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.denseblock3(x)\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "      # 3. metrics property\n",
        "  @property\n",
        "  def metrics(self):\n",
        "      return self.metrics_list\n",
        "      # return a list with all metrics in the model\n",
        "\n",
        "\n",
        "\n",
        "  # 4. reset all metrics objects\n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "          metric.reset_states()\n",
        "\n",
        "\n",
        "    #train_step method\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      img1, label = data\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "          output = self(img1, training=True)\n",
        "          loss = self.loss_function(label, output)\n",
        "  \n",
        "          \n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      \n",
        "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "      \n",
        "      # update the state of the metrics according to loss\n",
        "      self.metrics[0].update_state(label, output)\n",
        "      self.metrics[1].update_state(loss)\n",
        "      \n",
        "      # return a dictionary with metric names as keys and metric results as values\n",
        "      return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "  # 6. test_step method\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "      img1, label = data\n",
        "      # same as train step (without parameter updates)\n",
        "      output = self(img1, training=False)\n",
        "      loss = self.loss_function(label, output)\n",
        "      self.metrics[0].update_state(label, output)\n",
        "      self.metrics[1].update_state(loss)\n",
        "      \n",
        "      return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "dense_model = DenselyConnectedCNN()\n",
        "try_model(dense_model, train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yle5uZNwQnyG"
      },
      "outputs": [],
      "source": [
        "def create_summary_writers(config_name):\n",
        "    \n",
        "    # Define where to save the logs\n",
        "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
        "    # alternatively make a copy of the code that is used for later reference\n",
        "    \n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
        "    test_log_path = f\"logs/{config_name}/{current_time}/test\"\n",
        "\n",
        "    # log writer for training metrics\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "    # log writer for validation metrics\n",
        "    test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "    \n",
        "    return train_summary_writer, test_summary_writer\n",
        "\n",
        "train_summary_writer, test_summary_writer = create_summary_writers(config_name=\"RUN1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ-ORtNzQzxu"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "def training_loop(model, train_ds, test_ds, start_epoch,\n",
        "                  epochs, train_summary_writer, \n",
        "                  test_summary_writer, save_path):\n",
        "  \n",
        "    \n",
        "\n",
        "    test_accuracies = [] \n",
        "    test_losses = [] \n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "\n",
        "    # 1. iterate over epochs\n",
        "    for e in range(start_epoch, epochs):\n",
        "\n",
        "        # 2. train steps on all batches in the training data\n",
        "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "            metrics = model.train_step(data)\n",
        "\n",
        "        # 3. log and print training metrics\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            # for scalar metrics:\n",
        "            for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n",
        "            # alternatively, log metrics individually (allows for non-scalar metrics such as tf.keras.metrics.MeanTensor)\n",
        "            # e.g. tf.summary.image(name=\"mean_activation_layer3\", data = metrics[\"mean_activation_layer3\"],step=e)\n",
        "\n",
        "\n",
        "        \n",
        "        #print the metrics\n",
        "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "      \n",
        "        for key, value in metrics.items():\n",
        "          if key == \"acc\":\n",
        "            train_accuracies.append(value.numpy())\n",
        "          elif key == \"loss\":\n",
        "            train_losses.append(value.numpy())\n",
        "\n",
        "        # 4. reset metric objects\n",
        "        model.reset_metrics()\n",
        "\n",
        "\n",
        "        # 5. evaluate on test data\n",
        "        for data in test_ds:\n",
        "            metrics = model.test_step(data)\n",
        "        \n",
        "\n",
        "        # 6. log test metrics\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            # for scalar metrics:\n",
        "            for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n",
        "            # alternatively, log metrics individually (allows for non-scalar metrics such as tf.keras.metrics.MeanTensor)\n",
        "            # e.g. tf.summary.image(name=\"mean_activation_layer3\", data = metrics[\"mean_activation_layer3\"],step=e)\n",
        "            \n",
        "        print([f\"test_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        \n",
        "        for key, value in metrics.items():\n",
        "          if key == \"loss\":\n",
        "            test_losses.append(value.numpy())\n",
        "          elif key == \"acc\":\n",
        "            test_accuracies.append(value.numpy())\n",
        "       \n",
        "        # 7. reset metric objects\n",
        "        model.reset_metrics()\n",
        "        \n",
        "    # 8. save model weights if save_path is given\n",
        "    if save_path:\n",
        "        model.save_weights(save_path)\n",
        " \n",
        "    return test_losses,test_accuracies, train_losses,train_accuracies\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvE-rEwOOz3w"
      },
      "outputs": [],
      "source": [
        " #1. instantiate model\n",
        "model = BasicConv()\n",
        "\n",
        "# 2. choose a path to save the weights\n",
        "save_path = \"trained_model_RUN1\"\n",
        "\n",
        "test_losses, test_accuracies, train_losses,train_accuracies = training_loop(model=model,\n",
        "    train_ds=train_ds,\n",
        "    test_ds=test_ds,\n",
        "    start_epoch=0,\n",
        "    epochs=5,\n",
        "    train_summary_writer=train_summary_writer,\n",
        "    test_summary_writer=test_summary_writer,\n",
        "    save_path=save_path)\n",
        "\n",
        "#print(test_losses, test_accuracies, train_losses,train_accuracies)\n",
        "\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "line4,= plt.plot(train_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3,line4),(\"training_loss\",\"test_loss\",\"test accuracies\",\"train_accuracies\"))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLlyy0pSYIsY"
      },
      "outputs": [],
      "source": [
        "# open the tensorboard logs\n",
        "%tensorboard --logdir logs/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}